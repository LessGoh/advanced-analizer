import pandas as pd
import streamlit as st
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
from config import DATA_FIELDS


class DataValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö MPStats"""
    
    def __init__(self):
        self.validation_results = {}
        self.warnings = []
        self.errors = []
    
    def validate_all_data(self, loaded_data: Dict[str, pd.DataFrame]) -> Dict[str, dict]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        validation_results = {}
        
        for data_type, df in loaded_data.items():
            validation_results[data_type] = self._validate_by_type(df, data_type)
        
        self.validation_results = validation_results
        return validation_results
    
    def _validate_by_type(self, df: pd.DataFrame, data_type: str) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø—É"""
        
        if data_type == "trends":
            return self._validate_trends_data(df)
        elif data_type == "queries":
            return self._validate_queries_data(df)
        elif data_type == "price":
            return self._validate_price_data(df)
        elif data_type == "days":
            return self._validate_days_data(df)
        elif data_type == "products":
            return self._validate_products_data(df)
        else:
            return {"valid": True, "warnings": [], "errors": []}
    
    def _validate_trends_data(self, df: pd.DataFrame) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤"""
        warnings = []
        errors = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = DATA_FIELDS["trends"]
            missing_cols = []
            for field_key, col_name in required_columns.items():
                if col_name not in df.columns:
                    missing_cols.append(col_name)
            
            if missing_cols:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—ã
            if "–ú–µ—Å—è—Ü" in df.columns:
                invalid_dates = df["–ú–µ—Å—è—Ü"].isna().sum()
                if invalid_dates > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {invalid_dates} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
                if not df["–ú–µ—Å—è—Ü"].isna().all():
                    min_date = df["–ú–µ—Å—è—Ü"].min()
                    max_date = df["–ú–µ—Å—è—Ü"].max()
                    date_range = (max_date - min_date).days
                    
                    if date_range < 30:
                        warnings.append("–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω–µ–µ –º–µ—Å—è—Ü–∞ - –∞–Ω–∞–ª–∏–∑ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Ç–æ—á–Ω—ã–º")
                    elif date_range > 1095:  # 3 –≥–æ–¥–∞
                        warnings.append("–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ 3 –ª–µ—Ç - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            numeric_columns = ["–ü—Ä–æ–¥–∞–∂–∏", "–í—ã—Ä—É—á–∫–∞, ‚ÇΩ", "–¢–æ–≤–∞—Ä—ã", "–ë—Ä–µ–Ω–¥—ã"]
            for col in numeric_columns:
                if col in df.columns:
                    negative_values = (df[col] < 0).sum()
                    if negative_values > 0:
                        warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {negative_values} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ '{col}'")
                    
                    zero_values = (df[col] == 0).sum()
                    if zero_values > len(df) * 0.5:  # –ë–æ–ª–µ–µ 50% –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                        warnings.append(f"–ë–æ–ª–µ–µ 50% –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ '{col}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            if all(col in df.columns for col in ["–¢–æ–≤–∞—Ä—ã", "–¢–æ–≤–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏"]):
                inconsistent = (df["–¢–æ–≤–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏"] > df["–¢–æ–≤–∞—Ä—ã"]).sum()
                if inconsistent > 0:
                    errors.append(f"–ù–∞–π–¥–µ–Ω–æ {inconsistent} –∑–∞–ø–∏—Å–µ–π, –≥–¥–µ —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏ –±–æ–ª—å—à–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–≤–∞—Ä–æ–≤")
            
        except Exception as e:
            errors.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤: {str(e)}")
        
        return {
            "valid": len(errors) == 0,
            "warnings": warnings,
            "errors": errors,
            "records_count": len(df),
            "date_range": self._get_date_range(df, "–ú–µ—Å—è—Ü") if "–ú–µ—Å—è—Ü" in df.columns else None
        }
    
    def _validate_queries_data(self, df: pd.DataFrame) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        warnings = []
        errors = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ["–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ", "–ß–∞—Å—Ç–æ—Ç–∞ WB", "–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ"]
            missing_cols = [col for col in required_columns if col not in df.columns]
            
            if missing_cols:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            if "–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ" in df.columns:
                empty_keywords = df["–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"].isna().sum()
                if empty_keywords > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {empty_keywords} –ø—É—Å—Ç—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                duplicates = df["–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"].duplicated().sum()
                if duplicates > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {duplicates} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if "–ß–∞—Å—Ç–æ—Ç–∞ WB" in df.columns:
                zero_frequency = (df["–ß–∞—Å—Ç–æ—Ç–∞ WB"] == 0).sum()
                if zero_frequency > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {zero_frequency} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π —á–∞—Å—Ç–æ—Ç–æ–π")
                
                negative_frequency = (df["–ß–∞—Å—Ç–æ—Ç–∞ WB"] < 0).sum()
                if negative_frequency > 0:
                    errors.append(f"–ù–∞–π–¥–µ–Ω–æ {negative_frequency} –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π")
            
            if "–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ" in df.columns:
                zero_products = (df["–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ"] == 0).sum()
                if zero_products > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {zero_products} –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            if all(col in df.columns for col in ["–ß–∞—Å—Ç–æ—Ç–∞ WB", "–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ"]):
                effective_queries = len(df[df["–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"] >= 2.0])
                if effective_queries == 0:
                    warnings.append("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç ‚â• 2.0)")
                elif effective_queries < 10:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ –º–∞–ª–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {effective_queries}")
        
        except Exception as e:
            errors.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {str(e)}")
        
        return {
            "valid": len(errors) == 0,
            "warnings": warnings,
            "errors": errors,
            "records_count": len(df),
            "effective_queries": len(df[df["–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"] >= 2.0]) if "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ" in df.columns else 0
        }
    
    def _validate_price_data(self, df: pd.DataFrame) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Ü–µ–Ω–æ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        warnings = []
        errors = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ["–û—Ç", "–î–æ", "–í—ã—Ä—É—á–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä, ‚ÇΩ"]
            missing_cols = [col for col in required_columns if col not in df.columns]
            
            if missing_cols:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}")
                return {
                    "valid": False,
                    "warnings": warnings,
                    "errors": errors,
                    "records_count": len(df)
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω–æ–≤—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            if all(col in df.columns for col in ["–û—Ç", "–î–æ"]):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã–µ
                non_numeric_from = df['–û—Ç'].isna().sum()
                non_numeric_to = df['–î–æ'].isna().sum()
                
                if non_numeric_from > 0:
                    errors.append(f"–ù–∞–π–¥–µ–Ω–æ {non_numeric_from} –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ '–û—Ç'")
                
                if non_numeric_to > 0:
                    errors.append(f"–ù–∞–π–¥–µ–Ω–æ {non_numeric_to} –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ '–î–æ'")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                valid_rows = df.dropna(subset=['–û—Ç', '–î–æ'])
                if len(valid_rows) > 0:
                    invalid_ranges = (valid_rows["–û—Ç"] >= valid_rows["–î–æ"]).sum()
                    if invalid_ranges > 0:
                        errors.append(f"–ù–∞–π–¥–µ–Ω–æ {invalid_ranges} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ (–û—Ç >= –î–æ)")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
                    df_sorted = valid_rows.sort_values("–û—Ç")
                    overlaps = 0
                    for i in range(len(df_sorted) - 1):
                        if df_sorted.iloc[i]["–î–æ"] > df_sorted.iloc[i + 1]["–û—Ç"]:
                            overlaps += 1
                    
                    if overlaps > 0:
                        warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {overlaps} –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã—Ä—É—á–∫—É –Ω–∞ —Ç–æ–≤–∞—Ä
            if "–í—ã—Ä—É—á–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä, ‚ÇΩ" in df.columns:
                zero_revenue = (df["–í—ã—Ä—É—á–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä, ‚ÇΩ"] == 0).sum()
                if zero_revenue > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {zero_revenue} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –≤—ã—Ä—É—á–∫–æ–π –Ω–∞ —Ç–æ–≤–∞—Ä")
                
                negative_revenue = (df["–í—ã—Ä—É—á–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä, ‚ÇΩ"] < 0).sum()
                if negative_revenue > 0:
                    errors.append(f"–ù–∞–π–¥–µ–Ω–æ {negative_revenue} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π –≤—ã—Ä—É—á–∫–æ–π")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            if all(col in df.columns for col in ["–¢–æ–≤–∞—Ä—ã", "–¢–æ–≤–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏"]):
                valid_products_data = df.dropna(subset=["–¢–æ–≤–∞—Ä—ã", "–¢–æ–≤–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏"])
                if len(valid_products_data) > 0:
                    inconsistent = (valid_products_data["–¢–æ–≤–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏"] > valid_products_data["–¢–æ–≤–∞—Ä—ã"]).sum()
                    if inconsistent > 0:
                        errors.append(f"–ù–∞–π–¥–µ–Ω–æ {inconsistent} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º —Ç–æ–≤–∞—Ä–æ–≤")
        
        except Exception as e:
            errors.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Ü–µ–Ω–æ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {str(e)}")
        
        return {
            "valid": len(errors) == 0,
            "warnings": warnings,
            "errors": errors,
            "records_count": len(df),
            "price_range": (df["–û—Ç"].min(), df["–î–æ"].max()) if all(col in df.columns for col in ["–û—Ç", "–î–æ"]) and len(df) > 0 else None
        }
    
    def _validate_days_data(self, df: pd.DataFrame) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º"""
        warnings = []
        errors = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ["–î–∞—Ç–∞", "–û—Å—Ç–∞—Ç–æ–∫"]
            missing_cols = [col for col in required_columns if col not in df.columns]
            
            if missing_cols:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—ã
            if "–î–∞—Ç–∞" in df.columns:
                invalid_dates = df["–î–∞—Ç–∞"].isna().sum()
                if invalid_dates > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {invalid_dates} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å –¥–∞—Ç
                if not df["–î–∞—Ç–∞"].isna().all():
                    df_sorted = df.sort_values("–î–∞—Ç–∞")
                    date_gaps = 0
                    for i in range(len(df_sorted) - 1):
                        diff = (df_sorted.iloc[i + 1]["–î–∞—Ç–∞"] - df_sorted.iloc[i]["–î–∞—Ç–∞"]).days
                        if diff > 1:
                            date_gaps += 1
                    
                    if date_gaps > 0:
                        warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {date_gaps} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞—Ç–∞—Ö")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞—Ç–∫–∏
            if "–û—Å—Ç–∞—Ç–æ–∫" in df.columns:
                negative_stock = (df["–û—Å—Ç–∞—Ç–æ–∫"] < 0).sum()
                if negative_stock > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {negative_stock} –∑–∞–ø–∏—Å–µ–π —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏")
                
                zero_stock_days = (df["–û—Å—Ç–∞—Ç–æ–∫"] == 0).sum()
                total_days = len(df)
                if zero_stock_days > total_days * 0.3:  # –ë–æ–ª–µ–µ 30% –¥–Ω–µ–π –±–µ–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤
                    warnings.append(f"–ë–æ–ª–µ–µ 30% –¥–Ω–µ–π –±–µ–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤ –Ω–∞ —Å–∫–ª–∞–¥–µ ({zero_stock_days} –∏–∑ {total_days})")
        
        except Exception as e:
            errors.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º: {str(e)}")
        
        return {
            "valid": len(errors) == 0,
            "warnings": warnings,
            "errors": errors,
            "records_count": len(df),
            "date_range": self._get_date_range(df, "–î–∞—Ç–∞") if "–î–∞—Ç–∞" in df.columns else None
        }
    
    def _validate_products_data(self, df: pd.DataFrame) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤"""
        warnings = []
        errors = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ["SKU", "Final price", "Category position avg"]
            missing_cols = [col for col in required_columns if col not in df.columns]
            
            if missing_cols:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º SKU
            if "SKU" in df.columns:
                empty_sku = df["SKU"].isna().sum()
                if empty_sku > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {empty_sku} —Ç–æ–≤–∞—Ä–æ–≤ –±–µ–∑ SKU")
                
                duplicate_sku = df["SKU"].duplicated().sum()
                if duplicate_sku > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {duplicate_sku} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è SKU")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—ã
            if "Final price" in df.columns:
                zero_price = (df["Final price"] == 0).sum()
                if zero_price > 0:
                    warnings.append(f"–ù–∞–π–¥–µ–Ω–æ {zero_price} —Ç–æ–≤–∞—Ä–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π —Ü–µ–Ω–æ–π")
                
                negative_price = (df["Final price"] < 0).sum()
                if negative_price > 0:
                    errors.append(f"–ù–∞–π–¥–µ–Ω–æ {negative_price} —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π —Ü–µ–Ω–æ–π")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if "Search cpm avg" in df.columns:
                ads_products = (df["Search cpm avg"] > 0).sum()
                total_products = len(df)
                ads_percentage = (ads_products / total_products) * 100
                
                if ads_percentage < 10:
                    warnings.append(f"–ú–∞–ª–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å —Ä–µ–∫–ª–∞–º–æ–π: {ads_percentage:.1f}%")
                elif ads_percentage > 90:
                    warnings.append(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ —Å —Ä–µ–∫–ª–∞–º–æ–π: {ads_percentage:.1f}% - –≤–æ–∑–º–æ–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∑–∏—Ü–∏–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if "Category position avg" in df.columns:
                top_10 = (df["Category position avg"] <= 10).sum()
                top_100 = (df["Category position avg"] <= 100).sum()
                
                if top_10 == 0:
                    warnings.append("–ù–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ç–æ–ø-10 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
                if top_100 < 10:
                    warnings.append(f"–ú–∞–ª–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ç–æ–ø-100: {top_100}")
        
        except Exception as e:
            errors.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {str(e)}")
        
        return {
            "valid": len(errors) == 0,
            "warnings": warnings,
            "errors": errors,
            "records_count": len(df),
            "top_10_count": (df["Category position avg"] <= 10).sum() if "Category position avg" in df.columns else 0,
            "top_100_count": (df["Category position avg"] <= 100).sum() if "Category position avg" in df.columns else 0
        }
    
    def _get_date_range(self, df: pd.DataFrame, date_column: str) -> Optional[Tuple[str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç"""
        try:
            if date_column in df.columns and not df[date_column].isna().all():
                min_date = df[date_column].min()
                max_date = df[date_column].max()
                return (min_date.strftime("%Y-%m-%d"), max_date.strftime("%Y-%m-%d"))
        except:
            pass
        return None
    
    def get_validation_summary(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        total_errors = sum(len(result.get("errors", [])) for result in self.validation_results.values())
        total_warnings = sum(len(result.get("warnings", [])) for result in self.validation_results.values())
        
        return {
            "total_files": len(self.validation_results),
            "valid_files": sum(1 for result in self.validation_results.values() if result.get("valid", False)),
            "total_errors": total_errors,
            "total_warnings": total_warnings,
            "files_with_errors": [file_type for file_type, result in self.validation_results.items() if not result.get("valid", True)],
            "overall_valid": total_errors == 0
        }
    
    def display_validation_results(self):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ Streamlit"""
        if not self.validation_results:
            return
        
        summary = self.get_validation_summary()
        
        # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞
        if summary["overall_valid"]:
            st.success(f"‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é —É—Å–ø–µ—à–Ω–æ ({summary['total_files']} —Ñ–∞–π–ª–æ–≤)")
        else:
            st.error(f"‚ùå –ù–∞–π–¥–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ {len(summary['files_with_errors'])} —Ñ–∞–π–ª–∞—Ö")
        
        if summary["total_warnings"] > 0:
            st.warning(f"‚ö†Ô∏è –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {summary['total_warnings']}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É
        for file_type, result in self.validation_results.items():
            with st.expander(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {file_type}"):
                
                if result["valid"]:
                    st.success("‚úÖ –§–∞–π–ª –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                else:
                    st.error("‚ùå –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏")
                
                # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                st.info(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {result['records_count']}")
                
                if "date_range" in result and result["date_range"]:
                    st.info(f"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {result['date_range'][0]} ‚Äî {result['date_range'][1]}")
                
                # –û—à–∏–±–∫–∏
                if result["errors"]:
                    st.error("**–û—à–∏–±–∫–∏:**")
                    for error in result["errors"]:
                        st.error(f"‚Ä¢ {error}")
                
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                if result["warnings"]:
                    st.warning("**–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:**")
                    for warning in result["warnings"]:
                        st.warning(f"‚Ä¢ {warning}")
