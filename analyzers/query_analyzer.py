import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, List, Tuple, Optional
import streamlit as st


class QueryAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()
        self.prepare_data()
        
    def prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        self.data = self.data.dropna(subset=['–ß–∞—Å—Ç–æ—Ç–∞ WB', '–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'])
        self.data = self.data[self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'] > 0]
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–ø—Ä–æ—Å/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ' not in self.data.columns:
            self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'] = self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'] / self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        self.data['–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏'] = self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'].apply(self._categorize_efficiency)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∑–∞–ø—Ä–æ—Å–∞
        self.data['–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞'] = self._calculate_query_potential()
    
    def _categorize_efficiency(self, ratio: float) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        if ratio >= 5.0:
            return "–û—Ç–ª–∏—á–Ω–æ (‚â•5x)"
        elif ratio >= 3.0:
            return "–•–æ—Ä–æ—à–æ (3-5x)"
        elif ratio >= 2.0:
            return "–°—Ä–µ–¥–Ω–µ (2-3x)"
        else:
            return "–ù–∏–∑–∫–æ (<2x)"
    
    def _calculate_query_potential(self) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∑–∞–ø—Ä–æ—Å–∞ (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        freq_norm = self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'] / self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].max()
        ratio_norm = np.minimum(self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'] / 10, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–≤–µ—Ä—Ö—É
        
        # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª = —á–∞—Å—Ç–æ—Ç–∞ * –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        return (freq_norm * ratio_norm * 100).round(2)
    
    def get_efficiency_analysis(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        analysis = {
            "total_queries": len(self.data),
            "efficiency_breakdown": {},
            "effective_queries": {},
            "top_opportunities": [],
            "recommendations": []
        }
        
        try:
            # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            efficiency_counts = self.data['–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏'].value_counts()
            total = len(self.data)
            
            for category, count in efficiency_counts.items():
                percentage = (count / total) * 100
                analysis["efficiency_breakdown"][category] = {
                    "count": count,
                    "percentage": round(percentage, 2)
                }
            
            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç >= 2.0)
            effective_df = self.data[self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'] >= 2.0]
            analysis["effective_queries"] = {
                "count": len(effective_df),
                "percentage": round((len(effective_df) / total) * 100, 2),
                "avg_frequency": round(effective_df['–ß–∞—Å—Ç–æ—Ç–∞ WB'].mean(), 0) if len(effective_df) > 0 else 0,
                "avg_ratio": round(effective_df['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'].mean(), 2) if len(effective_df) > 0 else 0
            }
            
            # –¢–æ–ø –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (–≤—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª + –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç >= 2)
            top_opportunities = self.data[
                (self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'] >= 2.0) &
                (self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'] >= self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].quantile(0.7))  # –¢–æ–ø 30% –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            ].nlargest(20, '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞')
            
            analysis["top_opportunities"] = top_opportunities[[
                '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ', '–ß–∞—Å—Ç–æ—Ç–∞ WB', '–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ', 
                '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ', '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞'
            ]].to_dict('records')
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            analysis["recommendations"] = self._generate_efficiency_recommendations(analysis)
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def get_competition_analysis(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        analysis = {
            "competition_levels": {},
            "market_saturation": {},
            "niche_gaps": [],
            "recommendations": []
        }
        
        try:
            # –ê–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ
            self.data['–£—Ä–æ–≤–µ–Ω—å_–∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏'] = pd.cut(
                self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'],
                bins=[0, 10, 50, 200, float('inf')],
                labels=['–ù–∏–∑–∫–∞—è (‚â§10)', '–°—Ä–µ–¥–Ω—è—è (11-50)', '–í—ã—Å–æ–∫–∞—è (51-200)', '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (>200)'],
                include_lowest=True
            )
            
            competition_stats = self.data.groupby('–£—Ä–æ–≤–µ–Ω—å_–∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏').agg({
                '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': 'count',
                '–ß–∞—Å—Ç–æ—Ç–∞ WB': 'mean',
                '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ': 'mean'
            }).round(2)
            
            analysis["competition_levels"] = competition_stats.to_dict('index')
            
            # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞
            total_queries = len(self.data)
            low_competition = len(self.data[self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'] <= 50])
            high_frequency = len(self.data[self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'] >= self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].median()])
            
            analysis["market_saturation"] = {
                "low_competition_percentage": round((low_competition / total_queries) * 100, 2),
                "high_frequency_percentage": round((high_frequency / total_queries) * 100, 2),
                "market_status": self._determine_market_status(low_competition, high_frequency, total_queries)
            }
            
            # –ü–æ–∏—Å–∫ –Ω–∏—à —Å –Ω–∏–∑–∫–æ–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–µ–π –∏ –≤—ã—Å–æ–∫–∏–º —Å–ø—Ä–æ—Å–æ–º
            niche_gaps = self.data[
                (self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'] <= 30) &  # –ù–∏–∑–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è
                (self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'] >= self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].quantile(0.6))  # –í—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å
            ].nlargest(15, '–ß–∞—Å—Ç–æ—Ç–∞ WB')
            
            analysis["niche_gaps"] = niche_gaps[[
                '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ', '–ß–∞—Å—Ç–æ—Ç–∞ WB', '–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ', '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'
            ]].to_dict('records')
            
            analysis["recommendations"] = self._generate_competition_recommendations(analysis)
            
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def get_keyword_insights(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –∏–Ω—Å–∞–π—Ç—ã"""
        insights = {
            "keyword_stats": {},
            "length_analysis": {},
            "frequency_distribution": {},
            "semantic_groups": {},
            "recommendations": []
        }
        
        try:
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            insights["keyword_stats"] = {
                "total_unique_keywords": len(self.data),
                "avg_frequency": round(self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].mean(), 0),
                "median_frequency": round(self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].median(), 0),
                "avg_competition": round(self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'].mean(), 0),
                "median_competition": round(self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'].median(), 0)
            }
            
            # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            self.data['–î–ª–∏–Ω–∞_–∑–∞–ø—Ä–æ—Å–∞'] = self.data['–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ'].str.split().str.len()
            length_stats = self.data.groupby('–î–ª–∏–Ω–∞_–∑–∞–ø—Ä–æ—Å–∞').agg({
                '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': 'count',
                '–ß–∞—Å—Ç–æ—Ç–∞ WB': 'mean',
                '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ': 'mean'
            }).round(2)
            
            insights["length_analysis"] = length_stats.to_dict('index')
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
            freq_ranges = pd.cut(
                self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'],
                bins=[0, 1000, 10000, 100000, float('inf')],
                labels=['–ù–∏–∑–∫–∞—è (‚â§1K)', '–°—Ä–µ–¥–Ω—è—è (1-10K)', '–í—ã—Å–æ–∫–∞—è (10-100K)', '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (>100K)'],
                include_lowest=True
            )
            
            freq_distribution = freq_ranges.value_counts()
            insights["frequency_distribution"] = {
                range_name: count for range_name, count in freq_distribution.items()
            }
            
            # –ü—Ä–æ—Å—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ (–ø–æ –ø–µ—Ä–≤–æ–º—É —Å–ª–æ–≤—É)
            self.data['–ü–µ—Ä–≤–æ–µ_—Å–ª–æ–≤–æ'] = self.data['–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ'].str.split().str[0].str.lower()
            semantic_groups = self.data['–ü–µ—Ä–≤–æ–µ_—Å–ª–æ–≤–æ'].value_counts().head(10)
            
            insights["semantic_groups"] = {
                word: count for word, count in semantic_groups.items()
            }
            
            insights["recommendations"] = self._generate_keyword_recommendations(insights)
            
        except Exception as e:
            insights["error"] = str(e)
        
        return insights
    
    def create_queries_charts(self) -> Dict[str, go.Figure]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        charts = {}
        
        try:
            # 1. Scatter plot: –ß–∞—Å—Ç–æ—Ç–∞ vs –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è
            fig_scatter = px.scatter(
                self.data.head(1000),  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                x='–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ',
                y='–ß–∞—Å—Ç–æ—Ç–∞ WB',
                color='–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ',
                size='–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞',
                hover_data=['–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ'],
                title='–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º',
                labels={
                    '–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ (–∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è)',
                    '–ß–∞—Å—Ç–æ—Ç–∞ WB': '–ß–∞—Å—Ç–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (—Å–ø—Ä–æ—Å)',
                    '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ': '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç'
                },
                color_continuous_scale='RdYlGn'
            )
            fig_scatter.update_layout(height=500)
            charts["demand_supply_scatter"] = fig_scatter
            
            # 2. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
            efficiency_counts = self.data['–ö–∞—Ç–µ–≥–æ—Ä–∏—è_—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏'].value_counts()
            fig_efficiency = px.bar(
                x=efficiency_counts.index,
                y=efficiency_counts.values,
                title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏',
                labels={'x': '–ö–∞—Ç–µ–≥–æ—Ä–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'y': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤'},
                color=efficiency_counts.values,
                color_continuous_scale='RdYlGn'
            )
            fig_efficiency.update_layout(height=400)
            charts["efficiency_distribution"] = fig_efficiency
            
            # 3. –¢–æ–ø-20 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É
            top_potential = self.data.nlargest(20, '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞')
            fig_top_potential = px.bar(
                top_potential,
                x='–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞',
                y='–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ',
                orientation='h',
                title='–¢–æ–ø-20 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É',
                labels={'–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞': '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –∑–∞–ø—Ä–æ—Å–∞', '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ'},
                color='–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ',
                color_continuous_scale='RdYlGn'
            )
            fig_top_potential.update_layout(height=600)
            charts["top_potential_queries"] = fig_top_potential
            
            # 4. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            if '–î–ª–∏–Ω–∞_–∑–∞–ø—Ä–æ—Å–∞' in self.data.columns:
                length_analysis = self.data.groupby('–î–ª–∏–Ω–∞_–∑–∞–ø—Ä–æ—Å–∞').agg({
                    '–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ': 'mean',
                    '–ß–∞—Å—Ç–æ—Ç–∞ WB': 'mean',
                    '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': 'count'
                }).reset_index()
                
                fig_length = make_subplots(
                    rows=1, cols=2,
                    subplot_titles=['–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –¥–ª–∏–Ω–µ', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –¥–ª–∏–Ω–µ']
                )
                
                fig_length.add_trace(
                    go.Bar(x=length_analysis['–î–ª–∏–Ω–∞_–∑–∞–ø—Ä–æ—Å–∞'], 
                          y=length_analysis['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'],
                          name='–°—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç'),
                    row=1, col=1
                )
                
                fig_length.add_trace(
                    go.Bar(x=length_analysis['–î–ª–∏–Ω–∞_–∑–∞–ø—Ä–æ—Å–∞'], 
                          y=length_analysis['–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ'],
                          name='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤'),
                    row=1, col=2
                )
                
                fig_length.update_layout(height=400, title_text="–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –¥–ª–∏–Ω–µ")
                charts["length_analysis"] = fig_length
            
            # 5. Heatmap –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
            # –°–æ–∑–¥–∞–µ–º –±–∏–Ω—ã –¥–ª—è —á–∞—Å—Ç–æ—Ç—ã –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
            freq_bins = pd.qcut(self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'], q=5, labels=['–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è', '–ù–∏–∑–∫–∞—è', '–°—Ä–µ–¥–Ω—è—è', '–í—ã—Å–æ–∫–∞—è', '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è'])
            comp_bins = pd.qcut(self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'], q=5, labels=['–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è', '–ù–∏–∑–∫–∞—è', '–°—Ä–µ–¥–Ω—è—è', '–í—ã—Å–æ–∫–∞—è', '–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è'])
            
            heatmap_data = pd.crosstab(freq_bins, comp_bins, values=self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'], aggfunc='mean')
            
            fig_heatmap = px.imshow(
                heatmap_data,
                title='–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏',
                labels={'x': '–£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏', 'y': '–ß–∞—Å—Ç–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤', 'color': '–°—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç'},
                color_continuous_scale='RdYlGn'
            )
            fig_heatmap.update_layout(height=400)
            charts["competition_heatmap"] = fig_heatmap
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
        
        return charts
    
    def _determine_market_status(self, low_competition: int, high_frequency: int, total: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Ä—ã–Ω–∫–∞"""
        low_comp_pct = (low_competition / total) * 100
        high_freq_pct = (high_frequency / total) * 100
        
        if low_comp_pct > 60 and high_freq_pct > 40:
            return "–ù–µ–¥–æ–Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ä—ã–Ω–æ–∫"
        elif low_comp_pct < 30 and high_freq_pct < 60:
            return "–ü–µ—Ä–µ–Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ä—ã–Ω–æ–∫"
        else:
            return "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä—ã–Ω–æ–∫"
    
    def _generate_efficiency_recommendations(self, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        recommendations = []
        
        try:
            effective_pct = analysis["effective_queries"]["percentage"]
            
            if effective_pct >= 20:
                recommendations.append("üü¢ –û—Ç–ª–∏—á–Ω–∞—è –Ω–∏—à–∞! –ú–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è")
            elif effective_pct >= 10:
                recommendations.append("üü° –•–æ—Ä–æ—à–∞—è –Ω–∏—à–∞ —Å —É–º–µ—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏")
            elif effective_pct >= 5:
                recommendations.append("üü† –°—Ä–µ–¥–Ω—è—è –Ω–∏—à–∞. –¢—Ä–µ–±—É–µ—Ç—Å—è —Ç–æ—á–µ—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏")
            else:
                recommendations.append("üî¥ –°–ª–æ–∂–Ω–∞—è –Ω–∏—à–∞. –ú–∞–ª–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if len(analysis["top_opportunities"]) > 10:
                recommendations.append(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(analysis['top_opportunities'])} –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è SEO")
            
            if effective_pct > 0:
                avg_ratio = analysis["effective_queries"]["avg_ratio"]
                if avg_ratio >= 4:
                    recommendations.append("üöÄ –í—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ - –±—ã—Å—Ç—Ä—ã–π —Ä–æ—Å—Ç –≤–æ–∑–º–æ–∂–µ–Ω")
        
        except Exception as e:
            recommendations.append(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        
        return recommendations
    
    def _generate_competition_recommendations(self, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏"""
        recommendations = []
        
        try:
            market_status = analysis["market_saturation"]["market_status"]
            low_comp_pct = analysis["market_saturation"]["low_competition_percentage"]
            
            if market_status == "–ù–µ–¥–æ–Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ä—ã–Ω–æ–∫":
                recommendations.append("üü¢ –†—ã–Ω–æ–∫ –Ω–µ–¥–æ–Ω–∞—Å—ã—â–µ–Ω - –æ—Ç–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Ö–æ–¥–∞")
            elif market_status == "–ü–µ—Ä–µ–Ω–∞—Å—ã—â–µ–Ω–Ω—ã–π —Ä—ã–Ω–æ–∫":
                recommendations.append("üî¥ –†—ã–Ω–æ–∫ –ø–µ—Ä–µ–Ω–∞—Å—ã—â–µ–Ω - –≤—ã—Å–æ–∫–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è")
            else:
                recommendations.append("üü° –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä—ã–Ω–æ–∫ - —É–º–µ—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è")
            
            if low_comp_pct > 50:
                recommendations.append("üìà –ú–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –Ω–∏–∑–∫–æ–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–µ–π")
            
            niche_gaps_count = len(analysis["niche_gaps"])
            if niche_gaps_count > 10:
                recommendations.append(f"üéØ –ù–∞–π–¥–µ–Ω–æ {niche_gaps_count} –Ω–∏—à —Å –Ω–∏–∑–∫–æ–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–µ–π –∏ –≤—ã—Å–æ–∫–∏–º —Å–ø—Ä–æ—Å–æ–º")
        
        except Exception as e:
            recommendations.append(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏: {e}")
        
        return recommendations
    
    def _generate_keyword_recommendations(self, insights: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        recommendations = []
        
        try:
            # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            if "length_analysis" in insights:
                best_length = max(insights["length_analysis"].items(), 
                                key=lambda x: x[1].get('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ', 0))
                recommendations.append(f"üìù –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞: {best_length[0]} —Å–ª–æ–≤")
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã
            if "semantic_groups" in insights:
                top_semantic = list(insights["semantic_groups"].keys())[:3]
                recommendations.append(f"üî§ –¢–æ–ø —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã: {', '.join(top_semantic)}")
        
        except Exception as e:
            recommendations.append(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        
        return recommendations
    
    def get_summary_metrics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
        try:
            effective_queries = len(self.data[self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'] >= 2.0])
            
            summary = {
                "total_queries": len(self.data),
                "effective_queries": effective_queries,
                "effectiveness_rate": round((effective_queries / len(self.data)) * 100, 2) if len(self.data) > 0 else 0,
                "avg_frequency": round(self.data['–ß–∞—Å—Ç–æ—Ç–∞ WB'].mean(), 0),
                "avg_competition": round(self.data['–¢–æ–≤–∞—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ'].mean(), 0),
                "best_ratio": round(self.data['–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç_—Å–ø—Ä–æ—Å_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'].max(), 2),
                "top_query": self.data.loc[self.data['–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª_–∑–∞–ø—Ä–æ—Å–∞'].idxmax(), '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ'] if not self.data.empty else "N/A"
            }
            
            return summary
            
        except Exception as e:
            return {"error": str(e)}
